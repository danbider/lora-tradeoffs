# LoRA Learns Less and Forgets Less

This minimal repo contains information for the paper ["LoRA Learns Less and Forgets Less"](https://arxiv.org/abs/2405.09673) (Biderman et al. TMLR, 2024).

Model checkpoints and LoRA adapters can be found here: [https://huggingface.co/LoRA-TMLR-2024](https://huggingface.co/LoRA-TMLR-2024)


| Setting | Link |
| --------| ------|
| Continued PRetraining - Code | [LoRA-TMLR-2024/continued-pretraining-code-starcoder-python](https://huggingface.co/collections/LoRA-TMLR-2024/continued-pretraining-code-starcoder-python-66f22ce3b26f416f21f58142) |
| Continued Pretraing - Math | TBD |
| Instruction Finetuning - Code | [LoRA-TMLR-2024/instruction-finetuning-code-magicoder-evol-instruct-110k](https://huggingface.co/collections/LoRA-TMLR-2024/instruction-finetuning-code-magicoder-evol-instruct-110k-66f224a800152f31e4942a3b) |
| Instruction Finetuning - Math | TBD |


----

5/15/2024 - v1 of the paper shared on arXiv
8/13/2024 - Paper [accepted to TMLR](https://openreview.net/forum?id=aloEru2qCG)
9/23/2024 - arXiv v2 updated (same as TMLR camera ready version)
9/24/2024 - Model checkpoints uploaded to HuggingFace (WIP)
